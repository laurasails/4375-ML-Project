{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXjUFBREYIDs",
        "outputId": "f8750a13-8f6f-4d28-fa25-e5af69ce3707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import streamlit as st\n",
        "import pandas as pd #helps w/ data manipulation\n",
        "import re\n",
        "import requests # helps send & receive response from web browswer\n",
        "import plotly\n",
        "import plotly.express as px\n",
        "import json # for graph plotting in website\n",
        "import nltk # NLTK VADER for sentiment analysis\n",
        "import yfinance as yf\n",
        "#from fireworks.client import Fireworks\n",
        "import webbrowser\n",
        "import base64\n",
        "import numpy as np\n",
        "nltk.downloader.download('vader_lexicon')\n",
        "from datetime import datetime\n",
        "#from streamlit_option_menu import option_menu\n",
        "from urllib.request import urlopen, Request\n",
        "from bs4 import BeautifulSoup as bs #important for data scraping\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from prophet import Prophet\n",
        "from prophet.plot import plot_plotly\n",
        "from plotly import graph_objs as go #plotly is an interactive graph\n",
        "from datetime import datetime, timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(['neg', 'neu', 'pos', 'compound'], axis=1, inplace=True)\n",
        "df.to_csv(\"headlines.csv\", index=False)\n",
        "df = pd.read_csv(\"headlines.csv\")"
      ],
      "metadata": {
        "id": "qxyU1ApGZ0Mc"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "dates = pd.date_range((datetime.today() - timedelta(days=5)).strftime(\"%Y-%m-%d\"), periods=100, freq='h')\n",
        "df = pd.DataFrame({\n",
        "        'date': dates,\n",
        "        'ticker': 'APPL',\n",
        "        'sentiment_score': np.random.rand(100)\n",
        "})"
      ],
      "metadata": {
        "id": "4pmE2ubCaCNA"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function: DISPLAYS SENTIMENT GRAPHS VISUALLY\n",
        "fig = px.bar(df, x='date', y='sentiment_score', title='Sentiment over Time')\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "-w9Ac5hFayOB",
        "outputId": "e023fc26-990d-4aaf-b300-5129b7424397"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"1858ffeb-c51f-41d1-9f86-94fd010fc559\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1858ffeb-c51f-41d1-9f86-94fd010fc559\")) {                    Plotly.newPlot(                        \"1858ffeb-c51f-41d1-9f86-94fd010fc559\",                        [{\"alignmentgroup\":\"True\",\"hovertemplate\":\"date=%{x}\\u003cbr\\u003esentiment_score=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#000001\",\"pattern\":{\"shape\":\"\"}},\"name\":\"\",\"offsetgroup\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"textposition\":\"auto\",\"x\":[\"2025-05-07T00:00:00\",\"2025-05-07T01:00:00\",\"2025-05-07T02:00:00\",\"2025-05-07T03:00:00\",\"2025-05-07T04:00:00\",\"2025-05-07T05:00:00\",\"2025-05-07T06:00:00\",\"2025-05-07T07:00:00\",\"2025-05-07T08:00:00\",\"2025-05-07T09:00:00\",\"2025-05-07T10:00:00\",\"2025-05-07T11:00:00\",\"2025-05-07T12:00:00\",\"2025-05-07T13:00:00\",\"2025-05-07T14:00:00\",\"2025-05-07T15:00:00\",\"2025-05-07T16:00:00\",\"2025-05-07T17:00:00\",\"2025-05-07T18:00:00\",\"2025-05-07T19:00:00\",\"2025-05-07T20:00:00\",\"2025-05-07T21:00:00\",\"2025-05-07T22:00:00\",\"2025-05-07T23:00:00\",\"2025-05-08T00:00:00\",\"2025-05-08T01:00:00\",\"2025-05-08T02:00:00\",\"2025-05-08T03:00:00\",\"2025-05-08T04:00:00\",\"2025-05-08T05:00:00\",\"2025-05-08T06:00:00\",\"2025-05-08T07:00:00\",\"2025-05-08T08:00:00\",\"2025-05-08T09:00:00\",\"2025-05-08T10:00:00\",\"2025-05-08T11:00:00\",\"2025-05-08T12:00:00\",\"2025-05-08T13:00:00\",\"2025-05-08T14:00:00\",\"2025-05-08T15:00:00\",\"2025-05-08T16:00:00\",\"2025-05-08T17:00:00\",\"2025-05-08T18:00:00\",\"2025-05-08T19:00:00\",\"2025-05-08T20:00:00\",\"2025-05-08T21:00:00\",\"2025-05-08T22:00:00\",\"2025-05-08T23:00:00\",\"2025-05-09T00:00:00\",\"2025-05-09T01:00:00\",\"2025-05-09T02:00:00\",\"2025-05-09T03:00:00\",\"2025-05-09T04:00:00\",\"2025-05-09T05:00:00\",\"2025-05-09T06:00:00\",\"2025-05-09T07:00:00\",\"2025-05-09T08:00:00\",\"2025-05-09T09:00:00\",\"2025-05-09T10:00:00\",\"2025-05-09T11:00:00\",\"2025-05-09T12:00:00\",\"2025-05-09T13:00:00\",\"2025-05-09T14:00:00\",\"2025-05-09T15:00:00\",\"2025-05-09T16:00:00\",\"2025-05-09T17:00:00\",\"2025-05-09T18:00:00\",\"2025-05-09T19:00:00\",\"2025-05-09T20:00:00\",\"2025-05-09T21:00:00\",\"2025-05-09T22:00:00\",\"2025-05-09T23:00:00\",\"2025-05-10T00:00:00\",\"2025-05-10T01:00:00\",\"2025-05-10T02:00:00\",\"2025-05-10T03:00:00\",\"2025-05-10T04:00:00\",\"2025-05-10T05:00:00\",\"2025-05-10T06:00:00\",\"2025-05-10T07:00:00\",\"2025-05-10T08:00:00\",\"2025-05-10T09:00:00\",\"2025-05-10T10:00:00\",\"2025-05-10T11:00:00\",\"2025-05-10T12:00:00\",\"2025-05-10T13:00:00\",\"2025-05-10T14:00:00\",\"2025-05-10T15:00:00\",\"2025-05-10T16:00:00\",\"2025-05-10T17:00:00\",\"2025-05-10T18:00:00\",\"2025-05-10T19:00:00\",\"2025-05-10T20:00:00\",\"2025-05-10T21:00:00\",\"2025-05-10T22:00:00\",\"2025-05-10T23:00:00\",\"2025-05-11T00:00:00\",\"2025-05-11T01:00:00\",\"2025-05-11T02:00:00\",\"2025-05-11T03:00:00\"],\"xaxis\":\"x\",\"y\":[0.5488135039273248,0.7151893663724195,0.6027633760716439,0.5448831829968969,0.4236547993389047,0.6458941130666561,0.4375872112626925,0.8917730007820798,0.9636627605010293,0.3834415188257777,0.7917250380826646,0.5288949197529045,0.5680445610939323,0.925596638292661,0.07103605819788694,0.08712929970154071,0.02021839744032572,0.832619845547938,0.7781567509498505,0.8700121482468192,0.978618342232764,0.7991585642167236,0.46147936225293185,0.7805291762864555,0.11827442586893322,0.6399210213275238,0.1433532874090464,0.9446689170495839,0.5218483217500717,0.4146619399905236,0.26455561210462697,0.7742336894342167,0.45615033221654855,0.5684339488686485,0.018789800436355142,0.6176354970758771,0.6120957227224214,0.6169339968747569,0.9437480785146242,0.6818202991034834,0.359507900573786,0.43703195379934145,0.6976311959272649,0.06022547162926983,0.6667667154456677,0.6706378696181594,0.2103825610738409,0.1289262976548533,0.31542835092418386,0.3637107709426226,0.5701967704178796,0.43860151346232035,0.9883738380592262,0.10204481074802807,0.2088767560948347,0.16130951788499626,0.6531083254653984,0.2532916025397821,0.4663107728563063,0.24442559200160274,0.15896958364551972,0.11037514116430513,0.6563295894652734,0.1381829513486138,0.1965823616800535,0.3687251706609641,0.8209932298479351,0.09710127579306127,0.8379449074988039,0.09609840789396307,0.9764594650133958,0.4686512016477016,0.9767610881903371,0.604845519745046,0.7392635793983017,0.039187792254320675,0.2828069625764096,0.1201965612131689,0.29614019752214493,0.11872771895424405,0.317983179393976,0.41426299451466997,0.06414749634878436,0.6924721193700198,0.5666014542065752,0.2653894909394454,0.5232480534666997,0.09394051075844168,0.5759464955561793,0.9292961975762141,0.31856895245132366,0.6674103799636817,0.13179786240439217,0.7163272041185655,0.2894060929472011,0.18319136200711683,0.5865129348100832,0.020107546187493552,0.8289400292173631,0.004695476192547066],\"yaxis\":\"y\",\"type\":\"bar\"}],                        {\"template\":{\"data\":{\"candlestick\":[{\"decreasing\":{\"line\":{\"color\":\"#000033\"}},\"increasing\":{\"line\":{\"color\":\"#000032\"}},\"type\":\"candlestick\"}],\"contourcarpet\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"contourcarpet\"}],\"contour\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"contour\"}],\"heatmap\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"heatmap\"}],\"histogram2d\":[{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"type\":\"histogram2d\"}],\"icicle\":[{\"textfont\":{\"color\":\"white\"},\"type\":\"icicle\"}],\"sankey\":[{\"textfont\":{\"color\":\"#000036\"},\"type\":\"sankey\"}],\"scatter\":[{\"marker\":{\"line\":{\"width\":0}},\"type\":\"scatter\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#000038\"},\"font\":{\"color\":\"#000037\"},\"line\":{\"color\":\"#000039\"}},\"header\":{\"fill\":{\"color\":\"#000040\"},\"font\":{\"color\":\"#000036\"},\"line\":{\"color\":\"#000039\"}},\"type\":\"table\"}],\"waterfall\":[{\"connector\":{\"line\":{\"color\":\"#000036\",\"width\":2}},\"decreasing\":{\"marker\":{\"color\":\"#000033\"}},\"increasing\":{\"marker\":{\"color\":\"#000032\"}},\"totals\":{\"marker\":{\"color\":\"#000034\"}},\"type\":\"waterfall\"}]},\"layout\":{\"coloraxis\":{\"colorscale\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]]},\"colorscale\":{\"diverging\":[[0.0,\"#000021\"],[0.1,\"#000022\"],[0.2,\"#000023\"],[0.3,\"#000024\"],[0.4,\"#000025\"],[0.5,\"#000026\"],[0.6,\"#000027\"],[0.7,\"#000028\"],[0.8,\"#000029\"],[0.9,\"#000030\"],[1.0,\"#000031\"]],\"sequential\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]],\"sequentialminus\":[[0.0,\"#000011\"],[0.1111111111111111,\"#000012\"],[0.2222222222222222,\"#000013\"],[0.3333333333333333,\"#000014\"],[0.4444444444444444,\"#000015\"],[0.5555555555555556,\"#000016\"],[0.6666666666666666,\"#000017\"],[0.7777777777777778,\"#000018\"],[0.8888888888888888,\"#000019\"],[1.0,\"#000020\"]]},\"colorway\":[\"#000001\",\"#000002\",\"#000003\",\"#000004\",\"#000005\",\"#000006\",\"#000007\",\"#000008\",\"#000009\",\"#000010\"]}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"date\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"sentiment_score\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Sentiment over Time\"},\"barmode\":\"relative\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1858ffeb-c51f-41d1-9f86-94fd010fc559');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_hourly_sentiment(df, ticker):\n",
        "        # Group by date and ticker columns from df and calculate the mean\n",
        "        mean_scores = df.groupby(['date', 'ticker']).mean()\n",
        "\n",
        "        # Plot a bar chart with plotly\n",
        "        fig = px.bar(mean_scores, x=mean_scores.index.get_level_values(0), y='sentiment_score', title=ticker + ' Hourly Sentiment')\n",
        "        fig.update_xaxes(title_text='Hourly Sentiment')  # Update x-axis label\n",
        "        fig.update_yaxes(title_text='Sentiment Score')  # Update y-axis label\n",
        "        return fig"
      ],
      "metadata": {
        "id": "UeOWOCCRaMb0"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_daily_sentiment(df, ticker):\n",
        "        # Group by date and ticker columns from df and calculate the mean\n",
        "        mean_scores = df.groupby(['ticker', pd.Grouper(key='date', freq='D')]).mean().reset_index()\n",
        "\n",
        "        # Plot a bar chart with plotly\n",
        "        fig = px.bar(mean_scores, x='date', y='sentiment_score', title=ticker + ' Daily Sentiment')\n",
        "        fig.update_xaxes(title_text='Date')  # Update x-axis label\n",
        "        fig.update_yaxes(title_text='Sentiment Score')  # Update y-axis label\n",
        "        return fig"
      ],
      "metadata": {
        "id": "MP2jC6EHaOxr"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Scrape news\n",
        "def get_news(ticker):\n",
        "    finviz_url = f\"https://finviz.com/quote.ashx?t={ticker}\"\n",
        "    req = Request(url=finviz_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    response = urlopen(req)\n",
        "    html = bs(response, 'html.parser')\n",
        "    news_table = html.find(id='news-table')\n",
        "    return news_table"
      ],
      "metadata": {
        "id": "59r081IMAAqs"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parse News Table\n",
        "def parse_news(news_table):\n",
        "    rows = []\n",
        "    for row in news_table.find_all('tr'):\n",
        "        try:\n",
        "            headline = row.a.get_text()\n",
        "            td = row.td.text.strip().split()\n",
        "            if len(td) == 1:\n",
        "                time = td[0]\n",
        "            else:\n",
        "                date = td[0]\n",
        "                time = td[1]\n",
        "            rows.append([date, time, headline])\n",
        "        except:\n",
        "            continue\n",
        "    df = pd.DataFrame(rows, columns=['date', 'time', 'headline'])\n",
        "    df['date'] = df['date'].replace('Today', datetime.today().strftime('%Y-%m-%d'))\n",
        "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "3TCj_l2sAVFR"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score with VADER\n",
        "def score_news(parsed_news_df):\n",
        "    scores = [sia.polarity_scores(text) for text in parsed_news_df['headline']]\n",
        "    scores_df = pd.DataFrame(scores)\n",
        "    df_scored = parsed_news_df.join(scores_df)\n",
        "    df_scored.set_index('datetime', inplace=True)\n",
        "    df_scored.drop(['date', 'time'], axis=1, inplace=True)\n",
        "    return df_scored\n",
        "\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n"
      ],
      "metadata": {
        "id": "qGfVMnlgAYb_"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Label Sentiment for Classification\n",
        "def label_sentiment(score):\n",
        "    if score >= 0.05:\n",
        "        return 'positive'\n",
        "    elif score <= -0.05:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "df['sentiment_class'] = df['compound'].apply(label_sentiment)"
      ],
      "metadata": {
        "id": "1t2Mk2MZAbp0"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print everything\n",
        "ticker = 'AAPL'\n",
        "tableNews = get_news(ticker)\n",
        "# print(tableNews)\n",
        "parseNews = parse_news(tableNews)\n",
        "# print(parseNews)\n",
        "parseNews.to_csv('temp2.csv', index=False)\n",
        "df = score_news(parseNews)\n",
        "df.to_csv('temp.csv', index=False)\n",
        "#st.dataframe(df)\n",
        "\n",
        "# files.download('temp2.csv')  # For parseNews CSV\n",
        "# files.download('temp.csv')\n",
        "\n",
        "df['sentiment_class'] = df['compound'].apply(label_sentiment) # Now df has a compound column\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7dc4lBmbAiLI"
      },
      "execution_count": 187,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Naive Bayes Model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "# Prepare features and labels\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X = vectorizer.fit_transform(df['headline'])\n",
        "y = df['sentiment_class']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(\"Naive Bayes Accuracy using TF-IDF >>> \", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oplf0qMW9C7W",
        "outputId": "7bf2f426-adbb-4c30-e5bf-f2cd18640b7f"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Accuracy using TF-IDF >>>  0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  # BoW instead of TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Prepare features and labels using Bag of Words\n",
        "vectorizer = CountVectorizer(stop_words='english', max_features=1000)  # Changed to BoW\n",
        "X = vectorizer.fit_transform(df['headline'])\n",
        "y = df['sentiment_class']\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train and evaluate\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Results\n",
        "print(\"Naive Bayes Model Accuracy using BoW >>> \", accuracy_score(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYQGLdAV9fCk",
        "outputId": "889df986-1e58-4254-d7c2-c1cb8dff2f42"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes Model Accuracy using BoW >>>  0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Vectorize headlines using Bag of Words\n",
        "bow_vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
        "X_bow = bow_vectorizer.fit_transform(df['headline'])\n",
        "y = df['sentiment_class']\n",
        "\n",
        "# Split data\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm_bow = LinearSVC()\n",
        "svm_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = svm_bow.predict(X_test_bow)\n",
        "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
        "print(f\"SVM with Bag of Words Accuracy >>  {accuracy_bow:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhieKiOo-hIg",
        "outputId": "f2e92ba8-8eb9-4a55-fabc-25d000a6475b"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM with Bag of Words Accuracy >>  0.350\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Vectorize headlines using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['headline'])\n",
        "\n",
        "# Split data\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train SVM\n",
        "svm_tfidf = LinearSVC()\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
        "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
        "print(f\"SVM with TF-IDF Accuracy >>>  {accuracy_tfidf:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xde68OS--uxW",
        "outputId": "94ef9c97-678b-4b5a-9a11-64bc3d1f023c"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM with TF-IDF Accuracy >>>  0.300\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Vectorize headlines using Bag of Words\n",
        "bow_vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
        "X_bow = bow_vectorizer.fit_transform(df['headline'])\n",
        "y = df['sentiment_class']\n",
        "\n",
        "# Split data\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "logreg_bow = LogisticRegression(max_iter=1000)\n",
        "logreg_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = logreg_bow.predict(X_test_bow)\n",
        "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
        "print(f\"Logistic Regression with Bag of Words Accuracy >>> {accuracy_bow:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_ZFl-iO-6EC",
        "outputId": "f086e7bc-32b0-47e7-9289-0e9d09775524"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with Bag of Words Accuracy >>> 0.4500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Vectorize headlines using TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['headline'])\n",
        "\n",
        "# Split data\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Logistic Regression\n",
        "logreg_tfidf = LogisticRegression(max_iter=1000)\n",
        "logreg_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = logreg_tfidf.predict(X_test_tfidf)\n",
        "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
        "print(f\"Logistic Regression with TF-IDF Accuracy >>> {accuracy_tfidf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJIzNY0a-9mB",
        "outputId": "6c7ea42d-501d-4b5c-e65d-6b43848a8727"
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression with TF-IDF Accuracy >>> 0.2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# BoW vectorization\n",
        "bow_vectorizer = CountVectorizer(stop_words='english', max_features=1000)\n",
        "X_bow = bow_vectorizer.fit_transform(df['headline'])\n",
        "y = df['sentiment_class']\n",
        "\n",
        "# Train/test split\n",
        "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_bow, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_bow = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_bow.fit(X_train_bow, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_bow = rf_bow.predict(X_test_bow)\n",
        "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
        "print(f\"Random Forest with Bag of Words Accuracy >>> {accuracy_bow:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aUU4JWvskhR8",
        "outputId": "55f0e98f-9abf-4e16-a95d-a27c5d303fea"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with Bag of Words Accuracy >>> 0.5500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['headline'])\n",
        "y = df['sentiment_class']\n",
        "\n",
        "# Train/test split\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train Random Forest\n",
        "rf_tfidf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred_tfidf = rf_tfidf.predict(X_test_tfidf)\n",
        "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
        "print(f\"Random Forest with TF-IDF Accuracy >>> {accuracy_tfidf:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKaPYRBykkf1",
        "outputId": "957d39b8-9155-4451-fdea-5cbbedb8b999"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest with TF-IDF Accuracy >>> 0.4000\n"
          ]
        }
      ]
    }
  ]
}